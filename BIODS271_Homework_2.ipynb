{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlvh/biods271/blob/main/BIODS271_Homework_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xENMyaxuLmpt"
      },
      "source": [
        "# Assignment 2: Vision-Language Foundation Models\n",
        "Vision-language models (VLMs) jointly learn relationships between images and text. In this assignment, we will explore how VLMs can be used to perform a variety of reasoning tasks on medical images.\n",
        "\n",
        "❗Make sure to click File > Save a copy in Drive before you get started on this assignment. If you edit this notebook directly, your changes will not be saved.\n",
        "\n",
        "❗Before you get started, click Runtime > Change Runtime Type and select \"T4 GPU\". Then, click \"Connect\" in the upper right hand corner of this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkfU7cHuJAWe"
      },
      "source": [
        "### Install Python Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rp0e5NYZI999"
      },
      "outputs": [],
      "source": [
        "# Run this cell to install necessary Python packages\n",
        "!pip install open_clip_torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Python Packages"
      ],
      "metadata": {
        "id": "V336pNj6dkGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python packages that may be useful\n",
        "import open_clip\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "bdIvBpJddltS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBelEO9GMny6"
      },
      "source": [
        "## Part 1 [Coding Questions]: Exploring General-Domain Vision-Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wbOZtlEHDJ5"
      },
      "source": [
        "### Load OpenCLIP VLM (2 points)\n",
        "\n",
        "The [OpenCLIP](https://github.com/mlfoundations/open_clip) codebase provides access to a range of pretrained VLMs. Load the pretrained OpenCLIP ViT-B/16 model (id = laion2B-s34B-b88K) and compute the number of model parameters, the context length (i.e. the number of input tokens for the text encoder), and the vocabulary size of the text encoder. This VLM uses a Vision Transformer backbone for the image encoder and was pretrained on the 2 billion image-text pairs included in the LAION-2B dataset.\n",
        "\n",
        "**Expected Outputs:**\n",
        "- Number of trained parameters in an OpenCLIP ViT-B/16 model (Hint: this should be in the hundreds of millions!)\n",
        "- Context length for the text encoder\n",
        "- Vocabulary size for the text encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0tmiPVCBpho"
      },
      "outputs": [],
      "source": [
        "open_clip.create_model_and_transforms(?, pretrained=?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89UBxw0QQJK3"
      },
      "source": [
        "### Exploring CIFAR-100 (2 points)\n",
        "For the first part of this assignment, we will be working with the [CIFAR-100](https://paperswithcode.com/dataset/cifar-100) dataset, which includes objects from 100 classes.\n",
        "\n",
        "We'll begin by performing an exploratory analysis. Visualize any ten images from CIFAR-100.\n",
        "\n",
        "**Expected Output:**\n",
        "- Visualization of ten CIFAR-100 images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRzfVZkiVoxV"
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "cifar_100 = torchvision.datasets.CIFAR100(root='.', train=False, download=True)\n",
        "class_names = cifar_100.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg5qG8uOifOR"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usk-Ps3WLKAw"
      },
      "source": [
        "### Perform Zero-Shot Classification (10 points)\n",
        "\n",
        "Let's evaluate the OpenCLIP ViT-B/16 model by performing zero-shot classification on CIFAR-100.\n",
        "\n",
        "Use the OpenCLIP ViT-B/16 to encode each image and each label in CIFAR-100. For this task, please directly encode the provided CIFAR-100 class names as text and **do not** perform any prompt tuning or modifications to the label name.\n",
        "\n",
        "Hint: Is your code too slow? Make sure you're using the GPU!\n",
        "\n",
        "**Expected Outputs**\n",
        "- Zero-shot classification accuracy of the OpenCLIP ViT-B/16 model on CIFAR-100. Hint: Classification accuracy should be > 0.6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9pOUK6uMVoa"
      },
      "outputs": [],
      "source": [
        "# Set up dataloader for CIFAR100\n",
        "cifar_100 = torchvision.datasets.CIFAR100(root='.', train=False, download=True, transform=?) #Don't forget to fill in the transform!\n",
        "data_loader = DataLoader(cifar_100, batch_size=?, shuffle=False, drop_last=False)\n",
        "\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GclHMU3MRPD"
      },
      "source": [
        "### Prompt Engineering for VLMs (6 points)\n",
        "\n",
        "VLMs are sensitive to the input prompts. In the previous step, we directly encoded the labels as text (i.e. \"apple\" or \"cloud\"). However, using more descriptive prompts that encode labels as phrases or sentences (e.g. \"the apple\") can help improve model performance. Can you improve zero-shot classification performance by customizing prompts?\n",
        "\n",
        "Hint: Is it possible to ensemble a set of multiple prompts for a given class label? See the lecture slides from 04/23 for more information on prompt ensembles.\n",
        "\n",
        "**Expected Output**\n",
        "- Zero-shot classification accuracy of the OpenCLIP ViT-B/16 model on CIFAR-100 using your custom prompts. Make sure that your prompts contribute to performance improvements when compared to the results from the previous cell.\n",
        "\n",
        "Note that this is an open-ended question, and there are many possible solutions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RF0NONP1nhHv"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPkYn1ulMwbX"
      },
      "source": [
        "## Part 2 [Coding Questions]: Exploring Medical Vision-Language Models\n",
        "\n",
        "Let's now try a medical image dataset. We will use PatchCamelyon (PCam), a dataset consisting of color images extracted from histopathologic scans of lymph node sections. Each image is annotated with a binary label indicating presence of metastatic tissue (label of 1 = lymph node with metastasis; label of 0 = normal lymph node)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dibx4Gm7xCHk"
      },
      "outputs": [],
      "source": [
        "# Run the following cell to download PCam\n",
        "\n",
        "!gdown 1qV65ZqZvWzuIVthK8eVDhIwrbnsJdbg_\n",
        "!gdown 17BHrSrwWKjYsOgTMmoqrIjDy6Fa2o_gP\n",
        "\n",
        "# NOTE: The previous two lines of code may occasionally throw an error if too\n",
        "# many people have attempted to download the file within a short time-frame. If\n",
        "# you see this error, download the files manually from Google Drive and upload\n",
        "# to your Colab disk. Then, run the rest of this cell to format the files.\n",
        "# (File 1) https://drive.google.com/uc?id=1qV65ZqZvWzuIVthK8eVDhIwrbnsJdbg_\n",
        "# (File 2) https://drive.google.com/uc?id=17BHrSrwWKjYsOgTMmoqrIjDy6Fa2o_gP\n",
        "\n",
        "!gunzip camelyonpatch_level_2_split_test_x.h5.gz\n",
        "!gunzip camelyonpatch_level_2_split_test_y.h5.gz\n",
        "\n",
        "!mkdir pcam\n",
        "!mv camelyonpatch_level_2_split_test_x.h5 pcam\n",
        "!mv camelyonpatch_level_2_split_test_y.h5 pcam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOkAN278rznU"
      },
      "source": [
        "### Exploring PCam (2 points)\n",
        "We'll begin by performing an exploratory analysis. Visualize any ten images from PCam and describe any visual variabilities that you observe between the two classes.\n",
        "\n",
        "**Expected Output:**\n",
        "- Visualization of ten PCam images\n",
        "- Description of visual differences between images from the two classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "783tqBqXslv_"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "pcam = torchvision.datasets.PCAM(root='.', split='test', download=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fowovjZpvpMO"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description of Visual Differences Between Class=0 and Class=1**: [Your Answer Here]"
      ],
      "metadata": {
        "id": "s4NCsA6_YuOF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dT57Rzsr9Lg"
      },
      "source": [
        "### Perform Zero-Shot Classification with General-Domain VLM (4 points)\n",
        "\n",
        "Let's evaluate the OpenCLIP ViT-B/16 model by performing zero-shot binary classification on PCam.\n",
        "\n",
        "Use the OpenCLIP ViT-B/16 to encode each image and each label in PCam. Feel free to experiment with prompts of your choice.\n",
        "\n",
        "**Expected Outputs**\n",
        "- Zero-shot classification accuracy of the OpenCLIP ViT-B/16 model on Pcam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2P_UJOVz7du"
      },
      "outputs": [],
      "source": [
        "# Set up dataloader for PCam\n",
        "pcam = torchvision.datasets.PCAM(root='.', split='test', download=False, transform=?) #Don't forget to fill in the transform!\n",
        "data_loader = DataLoader(pcam, batch_size=?, shuffle=False, drop_last=False)\n",
        "\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3Enaj1isGhi"
      },
      "source": [
        "### Perform Zero-Shot Classification with a Biomedical VLM (6 points)\n",
        "\n",
        "Next, let's explore [BiomedCLIP](https://arxiv.org/abs/2303.00915), a medical VLM pretrained on a large collection of images and text from PubMed. Again, feel free to experiment with prompts of your choice.\n",
        "\n",
        "**Expected Outputs**:\n",
        "\n",
        "- Zero-shot classification accuracy of BiomedCLIP on PCam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcNe0O8P31VT"
      },
      "outputs": [],
      "source": [
        "from open_clip import create_model_from_pretrained, get_tokenizer\n",
        "\n",
        "# Load BiomedCLIP Weights\n",
        "model, preprocess = create_model_from_pretrained('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
        "tokenizer = get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
        "model.eval()\n",
        "\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6XWc9As1kQ9"
      },
      "source": [
        "## Part 3 [Open-Ended Exploration] - Evaluating FMs\n",
        "\n",
        "### Evaluations (10 points)\n",
        "Now that we have explored applications of the OpenCLIP ViT-B/16 and the BiomedCLIP VLMs on Pcam, let's perform an open-ended exploration in order to analyze the learned embedding spaces of various FMs. For this task, you will (1) generate image embeddings for each sample in the PCAM test set, (2) cluster the image-level embeddings, and (3) generate cluster visualizations. Details are provided below:\n",
        "1. *Generate image embeddings*: You will generate embeddings for each image in the PCAM test set.\n",
        "2. *Cluster image embeddings*: You will use a clustering algorithm of your choice (such as K-means) in order to cluster the embeddings. Hint: If this stage is too time-consuming, it might help to reduce the dimensionality of image embeddings via algorithms like PCA or UMAP.\n",
        "3. *Visualize clusters*: Generate plots that visualize the clusters. There are many possible options for visualizing clusters; feel free to choose any reasonable approach. Use the ground-truth labels associated with each sample to color each point in your plot.\n",
        "\n",
        "For full credit on this section, you must evaluate **at least 3** models. At least one of these models must be distinct from the OpenCLIP ViT-B/16 and BiomedCLIP VLMs explored earlier in this notebook. Some examples of models you may evaluate include PLIP (discussed in the 04/07 lecture), other OpenCLIP variants, and CONCH. You may also experiment with vision-only FMs, such as UNI."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Your code here"
      ],
      "metadata": {
        "id": "rBP81AjrbnUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y59fNuK826sP"
      },
      "source": [
        "### Written Analysis (8 points)\n",
        "\n",
        "Provide details on the open-ended experiments you conducted. In particular, provide justification for (1) the three (or more) models that you chose to evaluate, (2) the clustering algorithm and hyperparameters that you selected, and (3) the visualization algorithm you utilized. Then, summarize your key findings from your visualizations. Which models appear to generate the best image embeddings for this dataset? How can you tell?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Your answer here]"
      ],
      "metadata": {
        "id": "yOh-ix9QuyIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission Guidelines\n",
        "\n",
        "When you have completed this assignment, please export to PDF by selecting File > Print > Save as PDF. **Please confirm that the outputs of all code cells are visible in the PDF.** Then, upload your PDF on Canvas."
      ],
      "metadata": {
        "id": "A-0Lnl1vu0Ww"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XRyVkPuoY8LP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "nkfU7cHuJAWe",
        "V336pNj6dkGo"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}